---
title: "A2-Matrices y vectores aleatorios"
author: "Héctor San Román Caraza"
date: "4/10/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1.- Considere la matriz de datos siguiente:  X =  que consta de 3 observaciones (filas) y 3 variables (columnas)

```{r}
X = matrix(c(1,6,8,4,2,3,3,6,3), ncol =3)

b = matrix(c(1,1,1), ncol =3)

X1 = X[,1]
X2 = X[,2]
X3 = X[,3]


a1 = X1 + X2 + X3

a2 = X1 + (2*X2) - (3* X3)

```


### a) Hallar la media, varianza y covarianza de b'X y c'X 


```{r}
# Media
cat("Media b'X: ",mean(a1),'\n')

# Media
cat("Media c'X: ",mean(a2),'\n')

# varianza
cat("Varianza b'X: ",var(a1),'\n')

cat("Varianza c'X: ",var(a2),'\n')

#Covarianza 
cat("Covarianza b'X:",cov(matrix(c(8,14,14), ncol =1)),"\n")

cat("Covarianza c'X:",cov(matrix(c(0,-8,5), ncol =1)))

```
### b) Hallar el determinante de S (matriz de var-covarianzas de X)

```{r}
det(cov(X))
```
### c) Hallar la matriz de varianzas-covarianzas (o porqué no se puede hallar)

```{r}
cat("Covarianzas")
S = cov(X)
cat(S)
cat("Varianzas")
var(X)
```


### d) Hallar los valores y vectores propios de S

```{r}
lambda <- eigen(cov(S))
cat("Vectores propios")
lambda$vectors
cat("Valores propios","\n")
lambda$values
```


## 2.- Explore los resultados del siguiente código y dé una interpretación (se sugiere intersertarlo en un trozo de R en Rmarkdown para que dé varias ventanas de salida de resutados): 

```{r}
library(MVN)
x = rnorm(100, 10, 2)
y = rnorm(100, 10, 2)
datos = data.frame(x,y)
mvn(datos, mvnTest = "hz", multivariatePlot = "persp")
mvn(datos, mvnTest = "hz", multivariatePlot = "contour")
```

Aquí estamos haciendo un análisis de normal multivariable en nuestros datos. Estamos haciendo la prueba de Henze-Zirkler y de acuerdo a nuestro P value, sí tenemos una distribución normal multivariada. Vemos que el test Anderson-Darling también nos arroja que se trata de una normal multivariada, si observamos el p-value. Vemos que el código nos arrjoa 2 distintas gráficas, una de densidad y otra de contorno. La gráfica de densidad nos ayuda a ver la distribución de nuestros datos, mientras que la de contorno nos ayuda a ver las probabilidades dependiendo del nivel de los datos.

# 3.- Un periódico matutino enumera los siguientes precios de autos usados para un compacto extranjero con edad medida en años y precio en venta medido en miles de dólares. 

- x1: 1, 2, 3, 3, 4, 5, 6, 8, 9, 11

 - x2: 18.95, 19.00, 17.95, 15.54, 14.00, 12.95, 8.94, 7.49, 6.00, 3.99

## a) Construya un diagrama de dispersión 

```{r}
x1 <- c(1, 2, 3, 3, 4, 5, 6, 8, 9, 11)
x2 <- c(18.95, 19.00, 17.95, 15.54, 14.00, 12.95, 8.94, 7.49, 6.00, 3.99)
plot(x1,x2)
```

### b) Inferir el signo de la covarianza muestral a partir del gráfico. 

El signo que podemos inferir a partir de la gráfica será negativo, ya que muestra una tendencia lineal hacia abajo, al cuadrante 4 de nuestra gráfica. 

### c) Calcular el cuadrado de las distancias estadísticas    con     . Nota: para el cálculo de la distancia de Mahalanobis, usa: mahalanobis(A,medias,S).

```{r}

X3 = matrix(c(x1,x2), ncol =2)
S = cov(X3)

mahalanobis(X3, colMeans(X3), S)
```

